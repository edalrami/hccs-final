---
title: "Healthcare Final Project"
author: "Edwin Ramirez"
date: "May 3, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(ggplot2)
library(glmnet)
X_train = read.csv("X_train.csv")
X_test = read.csv("X_test.csv")
y_train = read.csv("y_train.csv", header=FALSE)
colnames(y_train) = c("post_total_cost")
y_test = read.csv("y_test.csv", header=FALSE)
colnames(y_test) = c("post_total_cost")
X = rbind(X_train, X_test)
y = rbind(y_train, y_test)
colnames(y) = c('post_total_cost')


cat_names = read.csv("cat_names.csv", header=TRUE)
cont_names = read.csv("cont_names.csv", header=TRUE)

categorical <- as.character(unlist(cat_names[1,]))
categorical

continuous <- as.character(unlist(cont_names[1,]))
continuous
```

# Linear Regression with LASSO Feature Selection


We see from the qq norm plot that our output variable `post_total_cost` is not normalized. Therefore, we transform the data by taking the log of our output variable.

```{r}
#Check normality of target variable
qqnorm(y_train$post_total_cost, pch = 1, frame = FALSE)

#normality condition is violated. Transform the data
qqnorm(log(y_train$post_total_cost), pch = 1, frame = FALSE)
qqline(log(y_train$post_total_cost), col = "steelblue", lwd = 2)


#Fit the lasso model to the data
grid = 10^seq(10,-2,length=100)

#X_train = cbind(X_train, y_train)
#colnames(X_train)
#any(is.na(X_train))

X_tr = model.matrix( ~ ., X_train)

lasso.mod = glmnet(X_tr, as.matrix(log(y_train)), alpha=1, lambda = grid)

set.seed(1)
cv.out = cv.glmnet(X_tr, as.matrix(log(y_train)), alpha=1 )
plot(cv.out)

#
#X_test = cbind(X_test, y_test)
X_ts = model.matrix( ~ ., X_test)

(bestlam = cv.out$lambda.min)

lasso.pred = predict(lasso.mod, s = bestlam, newx = X_ts)
mean((lasso.pred-y_test$post_total_cost)^2)


#X = cbind(X, y) 
X_ = model.matrix( ~ ., X)

out = glmnet(X_,as.matrix(log(y)),alpha=1, lambda = grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)
features = lasso.coef

features = data.frame(name = features@Dimnames[[1]][features@i + 1], coefficient = features@x)

##Go through each row and determine if a value is zero
row_sub = apply(features, 1, function(row) all(row !=0 ))
##Subset coefficient names to get selected features for linear regression model
selected_features = features[row_sub,]$name
selected_features
model_coef = features[row_sub,]$coefficient
```

Lasso regression gives the following features. We fit these features to a linear model, and analyze the adjusted R square of the model
```{r}
selected_features[2:49]

##FIT THESE VALUES TO A LINEAR MODEL#


lm_data = X_train[names(X_train)[names(X_train) %in% selected_features[2:49]]]

lm_data2 = cbind(y_train, lm_data)


cont_data = lm_data2[names(lm_data2)[!names(lm_data2) %in% categorical]]
cat_data = lm_data2[names(lm_data2)[!names(lm_data2) %in% continuous]]
dim(cat_data)

#normalize continuous features
scaled_cont = scale(cont_data)

lm_data3 = cbind(scaled_cont, cat_data)

#Create the linear model
lmfit = lm(post_total_cost ~ ., data=lm_data3)
summary(lmfit)

```

As we can see the model is not very useful. Save output of summary to text file
```{r}
#Write results of Linear model to txt
sink("model_outputs/lm.txt")
print(summary(lmfit))
sink()
```

# Classification

We decided to use hypothesis testing to select the features for the classifcation models. Whichever features we have selected from using stats tests on all of the relevant features, we will the use the ones with non-zero co-efficients to use for KNN, Naive Bayes, and Logistic Regression.

```{r}

##########################################################
#FEATURE SELETION FOR CLASSIFICATION - HYPOTHESIS TESTING
##########################################################



#d2 = read.csv("projectTrain.csv")

M = cbind(X,y)
d2 = M
#Subset for diabetic patients only

d2 = d2[d2$drug_class == '*ANTIDIABETICS*',]
no_rows = dim(d2)[1]
#number of training observations
tr = no_rows*0.8

#Test set of data
d2_ts = d2[tr:no_rows,]

#Training set of data
d2_tr = d2[1:tr,]

categorical
getChi <- function(var, d){
  ##Return the chi-test hypothesis result
  #
  #Parameters: 
  #d: Dataframe containing cont and cat data
  #var: The column that we'll be doing chi-test on
  #
  #Returns: A list containing the var name and the pvalue
  t = d%>%
    select('pdc_80_flag', toString(var)) %>%
    table() %>%
    chisq.test()
  return(list(var , t$p.value))
}


#Get chi square results
chi_results = lapply(categorical, getChi, d2_tr)

chi_df = as.data.frame(matrix(unlist(chi_results), nrow = length(unlist(chi_results[1]))))
chi_df = t(chi_df)
colnames(chi_df) = c("var", "pval")
chi_df = as.data.frame(chi_df)

chi_df$var = as.character(chi_df$var)
chi_df$pval = as.numeric(as.character(chi_df$pval))

#Selected Categorical features for classification
selected_cat_features = chi_df[(chi_df$pval < 0.05),]
dim(selected_cat_features)

###################################################################
#USE THESE FEATURES FOR KNN, NAIVE BAYES AND LOGISTIC REGRESSION
###################################################################

#Selected features for logistic regression
selected_features2 = chi_df[(chi_df$pval < 0.05),]$var
selected_features2 = as.array(selected_features2) 
```


```{r}
#### THESE ARE THE CAT FEATURES TO USE FOR CLASSIFICATION
selected_features2



#Separate the categorical and cont data
x_cats = d2_tr[,which(colnames(d2_tr) %in% selected_features2)]
x_cont = d2_tr[,which(colnames(d2_tr) %in% continuous)]
pdc_80_flag_ = d2_tr$pdc_80_flag

#Factorize categoricals
x_factors = model.matrix(pdc_80_flag ~ ., data=x_cats)[,-1]
dim(x_factors)

library(car)



#Get continuous features through hypothesis testing
getCont <- function(var, data){
   flag1 = data[data$pdc_80_flag == 1,]
   flag0 = data[data$pdc_80_flag == 0,]
   temp = as.data.frame(data[var])
   colnames(temp) = c("val")
   #Check for normality when pdc_80_flag is 1 and 0
   n1 = shapiro.test(flag1[,c(var)])
   n0 = shapiro.test(flag0[,c(var)])
   
   #If normality exists 
   if((n1$p.value > 0.05) & (n0$p.value > 0.05)){
     
     #Check for equal variance
     levene_ = leveneTest(temp$val ~ as.factor(data$pdc_80_flag))
     if(levene_$`Pr(>F)`[1] < 0.05){
       res = t.test(temp$val ~ data$pdc_80_flag, mu = 0, alternative = 'two.sided', conf = 0.95, var.eq = TRUE)
     }else{

       res = t.test(temp$val ~ data$pdc_80_flag, mu = 0, alternative = 'two.sided', conf = 0.95, var.eq = FALSE)
     }
     
   #If normality test doesnt pass then check medians
   }else{
     
     #Used for median comparison. Use if data is skewed
     res = wilcox.test(temp$val ~ data$pdc_80_flag, conf.int = T)
     
   }
  return(list(var, res$p.value))

}

test_stats_res = lapply(continuous, getCont, d2_tr)


test_res = as.data.frame(matrix(unlist(test_stats_res), nrow = length(unlist(test_stats_res[1]))))
test_res = t(test_res)
colnames(test_res) = c("var", "pval")
test_res = as.data.frame(test_res)

test_res$var = as.character(test_res$var)
test_res$pval = as.numeric(as.character(test_res$pval))

#Select continuous features
selected_cont_features = test_res[(test_res$pval < 0.05),]


selected_cat_features = selected_cat_features[complete.cases(selected_cat_features),]
selected_cat_features = selected_cat_features[selected_cat_features$var != "pdc_80_flag",]

#Create training set and test set
X2 = d2_tr[c(selected_cont_features$var, selected_cat_features$var, "pdc_80_flag")]
X3 = d2_ts[c(selected_cont_features$var, selected_cat_features$var, "pdc_80_flag")]


#Create model matrix compatible with glmnet
X2_ = model.matrix(pdc_80_flag ~ ., data = X2)[,-1]


glmmod = glmnet(X2_, y=as.factor(pdc_80_flag_), alpha=1, family = "binomial")
plot(glmmod, xvar = "lambda")


model_coef = as.data.frame(as.matrix(coef(glmmod)[,10]))
#Selected features will be adjust_total_30d, age_cat0, and age_grpN0 classifcation models
model_coef

bestlam2 = min(glmmod$lambda)

ix = which(model_coef[,1] != 0)
weights = model_coef[ix, 1]
weights



####################################
#KNN
####################################

#Of the selected features normalize continuous variables
#Which happens to only be adjust_total_30d

cls_feats = list("age_cat0", "age_grpN0", "adjust_total_30d")

cls_data = X2[names(X2)[names(X2) %in% cls_feats]]

cls_data$adjust_total_30d = (cls_data$adjust_total_30d - mean(cls_data$adjust_total_30d))/sd(cls_data$adjust_total_30d)
cls_data$age_cat0 = as.factor(cls_data$age_cat0)
cls_data$age_grpN0 = as.factor(cls_data$age_grpN0)
sapply(cls_data, class)



library(FastKNN)

cls_test = X3[names(X3)[names(X3) %in% cls_feats]]
cls_test$adjust_total_30d = (cls_test$adjust_total_30d - mean(cls_test$adjust_total_30d))/sd(cls_test$adjust_total_30d)
cls_test$age_cat0 = as.factor(cls_test$age_cat0)
cls_test$age_grpN0 = as.factor(cls_test$age_grpN0)
#Determine k neighbors to be used
k = as.integer(sqrt(dim(cls_data)[1]))
k
library(cluster)
#KNN with training set
dist1 = daisy(cls_data, metric = "gower")
knn_res_in = knn_training_function(cls_data, as.matrix(dist1), X2$pdc_80_flag, k=k) 
knn_res_in = as.integer(knn_res_in)
(in_accuracy = round(sum(knn_res_in == X2$pdc_80_flag)/length(knn_res_in),2))
(e_in = 1 - in_accuracy)


#KNN with test set
dist2 = Distance_for_KNN_test(cls_test, cls_data)  
knn_res_out = knn_test_function(cls_data, cls_test, dist2, X2$pdc_80_flag, k=k)
knn_res_out = as.integer(knn_res_out)
```

Accuracy rate of KNN
```{r}
(out_accuracy = round(sum(knn_res_out == X3$pdc_80_flag)/length(knn_res_out),2))
```

Classification Error of KNN
```{r}
#Classification error
(e_out = 1 - out_accuracy)
```


```{r}

######################################
#NaiveBayes
######################################
library(fastNaiveBayes)

cls_data$age_cat0 = as.integer(cls_data$age_cat0)
cls_data$age_grpN0 = as.integer(cls_data$age_grpN0)
distr = fastNaiveBayes.detect_distribution(cls_data, nrows = nrow(cls_data))
distr

nb_model = fastNaiveBayes.mixed(cls_data, as.factor(X2$pdc_80_flag), distribution = distr, laplace = 1)

#Model predictions
cls_test$age_cat0 = as.integer(cls_test$age_cat0)
cls_test$age_grpN0 = as.integer(cls_test$age_grpN0)


nb_pred = predict(nb_model, cls_test)
```

Accuracy of Naive Bayes
```{r}
(nb_acc = round(sum(nb_pred == X3$pdc_80_flag)/length(X3$pdc_80_flag),2))
```

Classificaiton Error of Naive Bayes
```{r}
#Classification error
(nb_e_out = 1 - nb_acc)
```


```{r}
model_results = matrix(c(out_accuracy, e_out, nb_acc, nb_e_out), ncol = 2)
colnames(model_results) = c("KNN", "NaiveBayes")
rownames(model_results) = c("AccuracyRate", "ErrorRate")
model_results = as.data.frame(model_results)
write.csv(model_results, "model_outputs/model_results.csv")
```